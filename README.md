ğŸš€ LLM-APM
Real-Time Observability for LLM-Powered Applications

Chatbot â€¢ Step-wise Latency â€¢ Token & Cost Tracking â€¢ Prometheus â€¢ Grafana

ğŸ§  Overview

LLM-APM is an end-to-end Application Performance Monitoring (APM) platform specifically designed for LLM applications.

This project goes beyond a basic /generate API and implements:

ğŸ’¬ A ChatGPT-style chatbot

ğŸ“Š A custom frontend performance dashboard

ğŸ§© A plug-and-play Python APM library (llm_apm)

ğŸ“ˆ Prometheus + Grafana observability stack

ğŸ§  Step-wise latency tracking via middleware & decorators

The goal is to make LLM systems observable, debuggable, and cost-aware.

ğŸ¯ Problem Statement

LLM applications are:

Latency-sensitive â±ï¸

Costly ğŸ’¸

Operationally opaque ğŸ•³ï¸

Traditional APM tools fail to answer:

Which internal step is slow?

How many tokens are used per request?

What is the real cost per interaction?

Are errors increasing over time?

ğŸ‘‰ LLM-APM solves this by introducing LLM-native observability.

âœ¨ Key Capabilities
ğŸ§  LLM Application (FastAPI)

Chatbot-based request handling

Central request lifecycle tracking

Middleware-driven latency measurement

Internal LLM configuration (no user-supplied max_tokens)

/metrics endpoint for Prometheus

ğŸ§© llm_apm Python Library

Reusable, installable APM library providing:

Request context propagation

Step-wise elapsed time tracking

Token usage aggregation

Cost estimation

Error classification

Prometheus metric exporters

Decorators for step instrumentation

@step("llm_api_call")
def call_llm(...):
    ...

ğŸ’¬ Frontend Chatbot (React + Vite)

ChatGPT-style conversational UI

Sends user messages to backend

No manual configuration from user

Automatically generates metrics

ğŸ“Š Frontend Dashboard

Custom-built UI showing:

Total requests

Average latency

Error rate

Total token usage

Per-request history

Expandable request-level details

This dashboard is not Grafana â€” it is a custom frontend, which is a major strength of this project.

ğŸ“ˆ Observability Stack

Prometheus â†’ Metrics collection

Grafana â†’ Time-series visualization & alerts

Supports:

1m / 5m / 1h / 24h latency trends

Error rate monitoring

Token & cost trends

ğŸ—ï¸ Architecture
User (Browser)
   â†“
React Frontend
   â”œâ”€â”€ Chatbot
   â””â”€â”€ Dashboard
   â†“
FastAPI Backend
   â”œâ”€â”€ APM Middleware
   â”œâ”€â”€ Step Decorators
   â”œâ”€â”€ LLM Client
   â””â”€â”€ /metrics
   â†“
Prometheus
   â†“
Grafana

âš™ï¸ Request Lifecycle (Step-wise Tracking)

User sends a chat message

Middleware starts overall timer

Steps executed:

Preprocessing

LLM API call

Response parsing

Metrics export

Tokens & cost calculated

Metrics exposed to Prometheus

Dashboards update in real time

ğŸ“‚ Project Structure (EXACT â€” FROM YOUR SCREENSHOT)
LLM-APM/
â”œâ”€â”€ .vscode/
â”‚   â””â”€â”€ settings.json
â”‚
â”œâ”€â”€ app/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ dependencies/
â”‚   â”œâ”€â”€ middleware/
â”‚   â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ routes/
â”‚   â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ requirements.txt
â”‚   â””â”€â”€ state.py
â”‚
â”œâ”€â”€ docker/
â”‚   â”œâ”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ Dockerfile_apm
â”‚
â”œâ”€â”€ frontend/
â”‚   â”œâ”€â”€ node_modules/
â”‚   â”œâ”€â”€ public/
â”‚   â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ .env
â”‚   â”œâ”€â”€ .gitignore
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â”œâ”€â”€ eslint.config.js
â”‚   â”œâ”€â”€ index.html
â”‚   â”œâ”€â”€ package-lock.json
â”‚   â”œâ”€â”€ package.json
â”‚   â”œâ”€â”€ postcss.config.js
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ tailwind.config.js
â”‚   â””â”€â”€ vite.config.js
â”‚
â”œâ”€â”€ llm_apm/
â”‚   â”œâ”€â”€ __pycache__/
â”‚   â”œâ”€â”€ llm_apm.egg-info/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py
â”‚   â”œâ”€â”€ context.py
â”‚   â”œâ”€â”€ cost.py
â”‚   â”œâ”€â”€ decorators.py
â”‚   â”œâ”€â”€ errors.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â”œâ”€â”€ middleware.py
â”‚   â”œâ”€â”€ README.md
â”‚   â””â”€â”€ utils.py
â”‚
â”œâ”€â”€ observability/
â”‚   â”œâ”€â”€ grafana/
â”‚   â”‚   â””â”€â”€ dashboards/
â”‚   â””â”€â”€ prometheus/
â”‚       â”œâ”€â”€ alert-rules.yml
â”‚       â”œâ”€â”€ alert.yml
â”‚       â”œâ”€â”€ prometheus.yml
â”‚       â””â”€â”€ recording-rules.yml
â”‚
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

ğŸ³ Containerization

Backend, frontend, Prometheus, and Grafana are containerized

docker-compose.yml orchestrates the full stack

Easy local startup & reproducibility

ğŸ“Š Metrics Exposed

Examples:

llm_requests_total

llm_request_latency_seconds

llm_step_latency_seconds

llm_tokens_total

llm_cost_usd_total

llm_errors_total

